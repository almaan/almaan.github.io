[{"authors":null,"categories":null,"content":"I\u0026rsquo;m a PhD student in the Lundeberg Lab, situated at SciLifeLab (Stockholm, Sweden) focusing on computational method development. I\u0026rsquo;ve always been fascinated with how mathematical models can be used to explain much of the richness found in complex systems (biological and non-biological) but also to enhance our understanding of their characteristics and behaviour; a fascination that - for the better or worse - tend to influence my research quite substantially. My second big passion is running, the farther the better, whether it\u0026rsquo;s for transportation or pure pleasure - nothing can quite compare to it.\n","date":1613814785,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1613814785,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/alma-anderson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alma-anderson/","section":"authors","summary":"I\u0026rsquo;m a PhD student in the Lundeberg Lab, situated at SciLifeLab (Stockholm, Sweden) focusing on computational method development. I\u0026rsquo;ve always been fascinated with how mathematical models can be used to explain much of the richness found in complex systems (biological and non-biological) but also to enhance our understanding of their characteristics and behaviour; a fascination that - for the better or worse - tend to influence my research quite substantially.","tags":null,"title":"Alma Anderson","type":"authors"},{"authors":["Alma Anderson"],"categories":[],"content":"Having covered two somewhat old papers \u0026ndash; at least by our field\u0026rsquo;s standards \u0026ndash; in the previous reviews, my idea was to select something a bit more \u0026ldquo;up to date\u0026rdquo; this time. The path from this ambition to ending up reviewing a 13 year old paper is not obvious, but also not completely random. The t-SNE technique that van Maaten and Hinton presented more than a decade ago is widely used and still spurs discussion. In 2019 Becht et.al made an argument about UMAP\u0026rsquo;s (Uniform Manifold Approximation and Projection) superiority to other methods for dimensional reduction, including t-SNE. And early this February (01-02-2021), Kobak et.al. presented a Matters Arising from Becht.et.al. where they argue that t-SNE is not inferior to UMAP when it comes to preserving global structure, if the two methods are initialized by the same procedure. New methods, heavily influenced by t-SNE, continue to emerge; for example this variatonal autoencoder by Graving and Couzin. The t-SNE paper has $17996$ citations (2021-02-20, Google Scholar), and would probably have more, had it not become so widely known that people stopped citing it (and rather cites the analysis suites used to apply it). To put that number into context, that\u0026rsquo;s about 4 citations a day, every day since published. With such an important paper, the temptation to review it became too large, and so here we are.\n Paper Title : Visualizing Data using t-SNE Authors : Laurens van der Maaten and Geoffrey Hinton Published : November 2008 Link : JMLR In short, this paper presents a technique for dimensionality reduction that is specifically designed to offer better visualization of high dimensional data while also requiring less parameter tuning and being competitive performance-wise. The method, t-SNE (t-distributed Stochastich Neighborhood Embedding), is actually a modification an the earlier SNE (Stochastich Neighborhood Embedding) method, proposed in 2002 by Hinton and Roweis and designed for the same purpose. SNE however, the authors argue, constructs fairly good visualizations of high dimensional data, but has two big drawbacks : (1) It does not allow for convex optimization, and has a inconvenient cost function which is sensitive to hyperparameter choice; (2) it suffers from what the authors call a \u0026ldquo;crowding\u0026rdquo; problem (more on this later).\nThey also mention, and compare, t-SNE to seven other methods for dimensionality reduction. One of the authors main motivations for developing t-SNE was that the currently available techniques failed to perform well on real world data. Linear techniques are know for being notoriously bad at maintaining close local relationships between data points, rather focusing on placing distant points far away. The pre-existing non-linear techniques sought to remedy this, but failed to preserve global and local structures when applied to real (noisy) data, even though performing well on artificial data.\nThe authors first present the older SNE technique to then discuss what sets t-SNE appart from this, a setup that I will follow as well. For convince, the same notation as the authors will be used: $x_i$ denotes data points in the higher dimensional space, $y_i$ in the lower dimensional space. Note, the $x_i$ values constitute our original data, the $y_i$ values are unknown, and what we seek to find the best fit for given our stated objective. We will also speak about probabilities, here $p$ and $q$ will be associated with high and low dimensional space respectively. As a final note, the authors use the terms \u0026ldquo;conditional\u0026rdquo; and \u0026ldquo;joint\u0026rdquo; probabilities in a kind of sloppy way, but I will keep their terminology for the sake of easy referencing.\nNow, the core concept in SNE/t-SNE is to first think of \u0026ldquo;closeness\u0026rdquo; between two data points in terms of how likely they are to pick each other as neighbors; a point could any pick another point as it\u0026rsquo;s neighbor, but the probability of these events differs. Then, we try to make the distributions over neighbors as similar as possible in high and low dimensional space, mathematically translating to minimizing their Kullback-Leibler Divergence (KLD).\nIn SNE the conditional probability that point $i$ would pick point $j$ as it\u0026rsquo;s neighbor is written as $p_{j|i}$ in high dimensional space, and $q_{j|i}$ in low dimensional space, with:\n$$ \\begin{array}{ll} \u0026amp;p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k\\neq i} \\exp(-||x_i - x_k||^2/2\\sigma_i^2)},\\quad p_{i|i} = 0 \\\\\n\u0026amp;q_{j|i} = \\frac{\\exp(-||y_i - y_j||^2 )}{\\sum_{k\\neq i} \\exp(-||y_i - y_k||^2)},\\quad q_{i|i} = 0 \\end{array} $$\nThe value $\\sigma_i$ is indiricely determined by the user and the data. The user provides a desired perplexity value ($\\rho$). Once the perplexity is given $\\sigma_i$ is given by the value that gives:\n$$ \\rho = 2^{H(P_i)}, \\quad H(P_i) = - \\sum_j p_{j|i}\\log_2 p_{j|i} $$\nThe $H(P_i)$ function is as you might notice the entropy of the neighborhood distribution for point $i$. Entropy can be interpreted in many ways, but here it\u0026rsquo;s helpful to think of it as the \u0026ldquo;peakiness\u0026rdquo; of your distribution, i.e., whether the probability mass is evenly spread or just found at a few concentrated sites. Meaning that, with a higher perplexity we have a higher entropy and the probability of picking a neighbor is more spread out, i.e., we expect more neighbors. In fact, the perplexity can be thought of as an estimate of the expected number of neighbors each point has, to see why this is, imagine you have nine points in a $3\\times 3$ grid, and the probability of that the center point $i$ picks one of the eight points as it\u0026rsquo;s neighbor is set to: $0$ for the corner points and $1/4$ for the remaining ones. Then the perplexity becomes:\n$$ P(P_i) = 2^{-H(P_i)}= 2^{-(4\\cdot 0 + 4\\cdot 0.25 * \\log_2 0.25)} = 2^{1\\cdot\\log_2(4)} = 4 $$\nWhich makes perfect sense, since we said that the center point only could pick the four non-corners as it\u0026rsquo;s neighbors. Actually, we could even plot the perplexity as a function of the corner points probability value:\nThe number of expected neighbors will then obviously be largest if all points have equal probability (that is, $\\frac{1}{8}$). For t-SNE the authors recommend to use perplexity values between 5-50, but this is highly dependent on the data.\nHaving clarified how we calculate $\\sigma_i$ and what the perplexity is, you might now ask why is there no $\\sigma$ in the definition of $q_{j|i}$? In fact there is, but it\u0026rsquo;s static and set to $1/\\sqrt{2}$. Hence ,since $(1/\\sqrt{2})^2 \\cdot 2 = 1$ we end up with the above expression for $q_{j|i}$.\nSo why are we using these forms for the conditional probabilities then? Well, the observant reader have probably noted the similarity of the probabilities to the pdf (probability density function) of the Univariate Gaussian. And indeed, there is a relationship between the two. The conditional probabilities above represent: \u0026ldquo;the probability that point $i$ would pick point $j$ as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at point $i$.\u0026quot;. This is makes sense, but has one disadvantage; the conditional probabilities (defined as above) are not symmetric, meaning that in general $p_{j|i} \\neq p_{i|j}$ (and the same for $q$). And why does this matter again? Well, it renders a cost function ($C$) that is hard to optimize:\n$$ \\begin{array}{l} \u0026amp;C = \\sum_i KL(P_i||Q_i) = \\sum_i\\sum_j p_{j|i}\\log \\frac{p_{j|i}}{q_{j|i}}\\rightarrow\\\\\n\u0026amp;\\rightarrow \\frac{\\partial C}{\\partial y_i} = 2 \\sum_j (p_{j|i} - q_{j|i} + p_{i|j}- q_{i|j})(y_i-y_j) \\end{array} $$\nOne intuitive solution to this issue is to simply use joint rather than conditional probabilities:\n$$ \\begin{array}{ll} \u0026amp;p_{ij} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_k\\sum_{l\\neq k} \\exp(-||x_k - x_l||^2/2\\sigma_i^2)},\\quad p_{ii} = 0 \\\\\n\u0026amp;q_{ij} = \\frac{\\exp(-||y_i - y_j||^2)}{\\sum_k\\sum_{l\\neq k} \\exp(-||y_k - y_l||^2)},\\quad q_{ii} = 0 \\end{array} $$\nThese joint probabilities are symmetric, and thus provide more friendly gradients. So, problem solved? No, unfortunately this doesn\u0026rsquo;t quite make the cut. The above definition of the joint probability, in high dimensional space, is sensitive to otuliers. If $x_i$ was an outlier it would have very small $p_{ij}$ values for all $x_j$\u0026rsquo;s, meaning its position wouldn\u0026rsquo;t be well determined by the remaining data. To circumvent this we discard the previous definition of the joint distribution (in high dimensional space) and instead use the (also symmetric) alternative:\n$$ p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}$$\nThis means that $\\sum_j p_{ij} \u0026gt; \\frac{1}{2n}$, which will ensure that every $x_i$ have a significant contribution to the cost function, meaning it\u0026rsquo;s placement in the low dimensional space actually matters now. Excellent.\nFinally, thanks to our symmetric joint probabilities we end up with \u0026ldquo;friendlier\u0026rdquo; gradients that are more convenient to use:\n$$ \\frac{\\partial C}{\\partial y_i} = 4\\sum_j (p_{ij} - q_{ij})(y_i - y_j) $$\nReducing the computational complexity of the gradients speeds up the learning process, but it does not avoid the second problem with SNE, crowding of data. In a nutshell, the crowding problem arise because we run out of space when we go from high to low dimensions. As the dimensions shrinks it becomes increasingly hard to keep moderately distant points away from each other while also making sure they reside near their closest neighbors. The authors put it very well in the sentence: \u0026quot;[..], in ten dimensions it is possible to have eleven data points that are mutually equidistant and there is no way to model this faithfully in a two-dimensional map.\u0026quot;. As a consequence data points at a moderate distance from $x_i$ are placed too far away; if this occurs for every data point, then it will eventually collapse the system into the center of the map.\nOther approaches had already been suggested to overcome the crowding problem, but the authors found these efficient, remarking on how \u0026ndash; in some methods \u0026ndash; early separation of point clusters rarely could be revoked, even though incorrect. Evidently, they saw room for improvements, and thus the idea of t-SNE was born.\nThe joint probabilities in t-SNE are given as:\n$$\\begin{array}{l} \u0026amp;p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}, \\quad p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_k\\sum_{l\\neq k} \\exp(-||x_k -x_l||^2/2\\sigma_i^2)}, \\quad p_{i|i} = 0 \\\\\n\u0026amp;q_{ij} = \\frac{(1+||y_i - y_j||^2)^{-1}}{\\sum_k\\sum_{l\\neq k} (1+||y_k-y_l||^2)^{-1}}, \\quad q_{ii}=0 \\end{array}$$\n$p_{ij}$ is the same as for the symmetric SNE, but the $q_{ij}$ expression has changed; and albeit similar to SNE, the modifications are imperative for the superiority of t-SNE. The new form of $q_{ii}$ also relates to a statistical distribution, namely the Student\u0026rsquo;s t-distribution with one degree of freedom (a.k.a. Cauchy distribution). The Cauchy distribution has the pdf:\n$$ p(x) = \\frac{1}{\\pi\\gamma}\\Bigg{[}1+ \\Big{(}\\frac{x-x_0}{\\gamma}\\Big{)}^2\\Bigg{]}^{-1}, \\quad \\gamma \u0026gt;0 $$\nFurthermore, the Cauchy distribution has heavier tails than the Gaussian distribution, which better accommodates for outlier values; this allows us to place points with moderate distances in the high-dimensional space very far away without having to worry about our system collapsing. Conveniently, we also get rid of the exponentials in the $q_{ij}$ expression, which reduces computational cost.\nGradients for t-SNE are given as:\n$$ \\frac{\\partial C}{\\partial y_i} 4 \\sum_j (p_{ij} - q_{ij})(y_i - y_j)(1+||y_i - y_j||^2)^{-1} $$\nThe authors derive this gradient in Appendix A, though I found their procedure a bit unclear and could recommend this resource for an alternative and very straightforward explanation.\nAs a consequence of the modifications from the old SNE technique, t-SNE manages to model both dissimilar data points as far apart and similar data points as nearby, which is just want we want.\nThe authors suggest to use a momentum term during the gradient descent procedure. The update being:\n$$ \\mathcal{Y}^{(t)} = \\mathcal{Y}^{(t-1)} + \\eta \\frac{\\partial C}{\\partial\\mathcal{Y}} + \\alpha(t)(\\mathcal{Y}^{t-1} - \\mathcal{Y}^{t-1}) $$\nWhere $\\mathcal{Y}$ is the complete set of lower dimensional points. They further suggest two \u0026ldquo;tricks\u0026rdquo; to improve the results. The first one being referred to as \u0026ldquo;early compression\u0026rdquo;, where they force map points ($y_i$) to stay together in the initial phase of the optimization, implemented by adding an $L_{2}$ penalty to the cost function (proportional to the sum of squared distances from the center). The second trick is \u0026ldquo;early exaggeration\u0026rdquo; where they \u0026ldquo;enlarge\u0026rdquo; the $p_{ij}$ value early on, which makes the $q_{ij}$ values too small to properly model their corresponding $p_{ij}$ value and thus encourage larger $q_{ij}$ values (when appropriate); the result being that tight clusters tend to form. Tight clusters means more open space, and increase the clusters' freedom of movement, allowing them to find a favorable global organization.\nOnce the theory behind t-SNE has been presented, the authors proceed to compare their method to seven others, using five different data sets. I will not really devote too much time to this as the results are best gauged by visual inspection and I don\u0026rsquo;t have the rights to reproduce any figures. The general trends are however obvious, clusters in the t-SNE plot are better separated, arrange more logically in relation to each other (e.g., in the MNIST data sets, the clusters for 3\u0026rsquo;s and 5\u0026rsquo;s are close as well as clusters of 9\u0026rsquo;s and 4\u0026rsquo;s), and they also captures the axes of variance very well. The image below shows a t-SNE visualization of some spatial transcriptomics data (30k dimensions) compressed into two dimensions using t-SNE, data points are colored according to expression levels of a certain marker.\nThe authors then continue to state that their method runs efficiently with up to $10000$ data points, but as the computational and memory complexity of t-SNE is $\\mathcal{O}(n^2)$, performance gets a bit shaky after this. Their solution is to only use a subset of the data (hopefully representative of the whole population). The authors \u0026ndash; and I fully agree that this is a must \u0026ndash; are very keen to still use the complete data set somehow to learn the correct character of the underlying manifold. Thus, what they do is that they compute their $p_{ij}$ values from the complete data set, but only embed a subset in the low dimensional space. To do this, they randomly select their landmark points (the subset that will be used), construct a neighborhood graph including all points, and simulate a random walk from each landmark; the random walk is terminated when a different landmark point is reached. The fraction of times landmark $i$ lands at landmark $j$ gives the conditional probability $p_{j|i}$. Illustrating their results on the MNIST data set (60000 total data points, 6000 in the subset), the results looks decent; but MNIST is also a relatively easy data set to work with.\nThe next part of the paper compares t-SNE to other methods for dimensionality reduction, I will actually skip this part - even though it is interesting - since a lot of the methods they compare to are a bit \u0026ldquo;out of date\u0026rdquo; and I don\u0026rsquo;t have enough knowledge about them to put them into proper context or evaluate the authors' claims. What I can say, is that the authors more or less claim superiority to all the other methods, being: Sammon mapping, Isomap, LLE, CCA, SNE, MVU, and Laplacian Eigenmaps. From the results and their discussion, it seems like they have good grounds for these claims.\nSomething i deem slighlty more interesting, is the last part of the paper, where weaknesses of t-SNE are examined. The authors themselves identify three potential weaknesses of their method:\n Dimensionality reduction for other purposes: the authors say that they have not examined performance of t-SNE when the low dimensional space is supposed to be higher than 2 or 3 dimensions. It\u0026rsquo;s likely that if we want the output to be in higher dimensions ($d\u0026gt;3$) a different degree of freedom is required for the Student\u0026rsquo;s t-distribution. Curse of intrinsinc dimensionality : since euclidean distances are used, t-SNE implicitly assumes a local linearity of the data manifold. If the data has a high intrinsic dimensionality and is highly variable, this assumption might not hold. They believe that the issue can be mitigated by using a pre-processing step that compresses the data more efficiently and reduces the variance, for example an autoencoder. Non-convexity of the t-SNE cost function : The cost function of t-SNE is still not convex (just like SNE), and thus requires a choice of several optimization parameters. Still, the authors show that t-SNE is not too sensitive to hyperparameter choice and that robust results are obtained. They also argue \u0026ndash; and I\u0026rsquo;m inclined to agree \u0026ndash; that a local optimum of a cost function fit for our objectives is better than a global optimum of a cost function that is ill-suited for our task. I guess the real question actually is how poor the cost function of other dimensionality reduction techniques really are compared to t-SNE\u0026rsquo;s.  And by that we have, with highly varying depth, walked through the different parts of the paper. It was a true pleasure reading the paper this thoroughly, it made me reflect and think quite a lot. t-SNE might be an \u0026ldquo;old technique\u0026rdquo; by now, but its simplicity is still appealing (to me t-SNE is way more intuitive than UMAP). Worth mentioning is that the technique have been \u0026ldquo;enhanced\u0026rdquo; several times since its birth, most prominent is perhaps the Barnes-Hut approximation that reduces run time to $\\mathcal{O}(N\\log N)$ (the scikit implementation uses this). For a record of t-SNE\u0026rsquo;s development throughout the years I would refer to Laurens van der Maaten\u0026rsquo;s webpage.\n","date":1613814785,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613814785,"objectID":"2d8815351327aa6b10891236fc46328f","permalink":"/post/2021-02-20-tsne/","publishdate":"2021-02-20T10:53:05+01:00","relpermalink":"/post/2021-02-20-tsne/","section":"post","summary":"Having covered two somewhat old papers \u0026ndash; at least by our field\u0026rsquo;s standards \u0026ndash; in the previous reviews, my idea was to select something a bit more \u0026ldquo;up to date\u0026rdquo; this time.","tags":["review","tsne","manifold","machine learning"],"title":"Paper Review | Visualizing Data using t-SNE","type":"post"},{"authors":["Alma Anderson"],"categories":[],"content":"For this bi-weekly paper review, my choice fell upon a paper that I\u0026rsquo;ve skimmed through multiple times before, but never gave the proper read-through it deserves. It was a definite given when I started to curate a list of papers that I wanted to include in this series of reviews. Not only do I find it to be an elegant and well-composed paper, but it has had nothing but an /imperative/ influence on the RNA-seq based analysis these last years. In the paper, they present a model for normalization (and variance stabilization) of single-cell RNA-seq data, which now has become widely adopted within the community. Almost every paper not focusing on method development that I\u0026rsquo;ve stumbled across, use the \u0026ldquo;sctransform\u0026rdquo; package or derivatives of it to normalize scRNA-seq data. Thus, without further ado let us begin.\n Paper Title : Normalization and variance stabilization of single-cell RNA-seq data using regularized negative binomial regression\nAuthors : Christoph Hafemeister and Rahul Satija\nPublished 23 December 2019\ndoi : https://doi.org/10.1186/s13059-019-1874-1\nReview : Anyone who has ever worked with single cell RNA-seq data knows that before any \u0026ldquo;flashy and cool\u0026rdquo; results can be extracted, we need to apply some pre-processing steps to our data. Aside from filtering for low quality observations or irrelevant features, the very first step - and perhaps most important - tends to be normalization of the data. Normalization is a dead given in any analysis workflow, but let us just take one step back and ask why? Why do we care about normalization, why can\u0026rsquo;t we just \u0026ldquo;jump right at it\u0026rdquo;?\nProbing our data with the right tools will allow us to identify cell types and cell states as well as cell-to-cell variations. However if we aren\u0026rsquo;t careful technical factors might confound these results, making us come to incorrect and false conclusions regarding the patterns in our data. One technical factor that has a huge influence on the result of our downstream analysis is the sequencing depth. For example, imagine that we have two cell types A and B, where cells of A in general have higher transcriptional activity than those of B. Now if we were to compare the (raw and unprocessed) expression of a houskeeping gene between cells from the two types, it would be listed as upregulated in A, even though we expect all housekeeping genes to be of approximately equal relative expression across cell types. This is of course undesireable, and we therefore attempt to normalize the data, in order to remove any technical variation while preferably maintaining biological differences.\nThe authors define two \u0026ldquo;objectives\u0026rdquo; or criteria, that should hold for a single-cell data set to be considered sucessfully normalized:\n Normalized expression should be uncorrelated with sequencing depth of cells. Variance of a normalized gene (across cells) should not be affected by gene abundance or sequencing depth.  In contrast to the two previously existing paradigms of using either cell-specific scaling factors or probabilistic approaches they consider their approach as more of a statistical approach, distinct from the others. I would perhaps say that their strategy could fit into to the set of probabilistic approaches, but that\u0026rsquo;s a minor detail.\nThe method they present builds on a generalized linear model (GLM) where they use a constrained negative binomial error model, with sequencing depth as a covariate, and a log-link function for the mean.\nAfter evaluating different error models such as: an unconstrained Poisson, a Negative Binomial and a Zero Inflated Negative Binomial (ZINB), they noticed that the Poisson was not flexible enough, while the unconstrained NB and ZINB models were prone to overfitting. Therefore, as is common in several bulk normalization methods, they decided to regularize the model parameters by pooling information from genes with similar mean expression values, resulting in a regularized negative binomial regression. The constrained part is one of the model\u0026rsquo;s key features, the other one being to include the sequencing depth as a covariate. We will soon come back to the regularization, but let us first just briefly look at their GLM model:\n$$ \\log(E[x_{gc}]) = \\log(\\mu_{gc}) = \\beta_{0g} + \\beta_{1g} \\log_{ 10 }(m_c) $$\nWhere $x_{gc}$ is the expression of gene $g$ in cell $c$, and $m_{c}$ is the library size used as a proxy for sequencing depth.\nIn short, this means that the authors make the implicit assumption that most genes are not differentially expressed; genes have a common baseline ($\\beta_{0g}$) and differences across cells will mainly be due to their varying library size (influence controlled by $\\beta_{1g}$).\nThe negative binomial has one more parameter that we need to pay attention to, the dispersion parameter $\\theta$ which gives us the variance of our data. That is, if:\n$$ x \\sim NB(\\mu,\\theta)$$\nthen\n$$E[x] = \\mu, \\quad Var[x] = \\mu + \\frac{\\mu^2}{\\theta}$$\nInterestingly, the authors convincingly show that if we were to learn one set of parameters for each gene, these will be severely overfitted. What really made me appreciate the prevalance of this overfitting, was the results they present in Figure 2 (included below).\nIn the top row they show the estimated parameter values plotted as a function of gene mean; in a good model we would expect to have similar parameter values for genes with similar mean values. In the bottom row, they fit their GLM model to a random subsets of the data and assessed the variance of the parameter values (in a bootstrap fashion), showing how the variance (red dots) was very high, especially for low and medium expressed genes.\nTo remedy this issue of overfitting, they regularize their model, though not using the regularization terms that perhaps most of our familiar with like a $l_1$ or $l_2$ penalty. Instead they pool information from several genes to make more robust estimates. They first fit one GLM to each gene, but then proceed to apply kernel regression to the resulting parameter estimates. The intent being to learn regularized parameters that depend on a gene\u0026rsquo;s average expression. The blue dots in panel B of the figure above show the variance after using this regularized approach; as you can see the variance in parameter estimates is significantly reduced.\nThough, we still have some work left to do before our data is normalized. Having obtained the set of regularized parameters, one applies this regression model to the data in order to compute the residuals ($r_{gc}$).\n$$ r_{gc} = x_{gc} - \\mu_{gc}, \\quad \\mu_{gc} = \\exp[\\beta_{0g}+ \\beta_{1g}\\log_{10}(m_c)] $$\nThe residuals represent the difference between the response estimated mean and the observed expression value. But as one might remember, for the negative binomial, the variance can differ between genes even though they have the same mean ($\\mu$), an effect of the dispersion parameter $\\theta$. Thus, we will not use the raw residuals, but what is known as the Pearson residuals ($z_{gc}$), which account for the gene-specific variation term:\n$$ z_{gc} = \\frac{r_{gc}}{\\sigma_{gc}}, \\quad \\sigma_{gc} = \\sqrt{\\mu_{gc} + \\frac{\\mu_{gc}^2}{\\theta_g}} $$\nThis transformation presents the residuals in units of standard deviations, correcting for the specific gene variance. Implementationwise they also clip their $z_{gc}$ values to be less than $\\sqrt{N}$ (where $N = $ number of cells), in order to \u0026ldquo;reduce the impact of extreme outliers\u0026rdquo;. This value seems to have been determined fairly empirically.\nThe Pearson residuals are treated as normalized expression levels; the idea being that we\u0026rsquo;ve now regressed out any contribution from the sequencing depth to the observed expression levels. Using Pearson residuals has one additional benefit, being that the transformation we apply to compute them is inherently variance-stabilizing.\nThe image below is a merge between Figure 1C-E and Figure 3A-B and requires a bit of explanation in order to make sense. The curves represent the expression values as a function of cell UMI count where the genes have been divided into six groups based on their average expression in the data set. The six groups are not equally sized, but represent bins with equal width spanning the expression range. The bottom bar graphs are slightly more complex; the same six gene groups are used, and the cells are divided into five equally-sized groups with increasing average sequencing depth (group 1 having the highest value). Next, they calculated the total variance in each gene to then see how much each cell group contributed to this value. In perfectly normalized data, one would expect to see an equal contribution (here $20\\%$) between the cell groups, as gene expression should be uncorrelated with sequencing depth (criterion 1 from their \u0026ldquo;objectives\u0026rdquo;).\nComparing the use of Pearson residuals with raw UMI counts and log-normaliztion, we observe the following:\n  Using raw UMI counts, a clear correlation between sequencing depth and gene expression in all ranges of gene expression is present. Log-normalization mitigate this issue among lowly expressed genes (group 4-5), but it persists in the high and medium expressed genes. The regularized negative binomial normalization renders values near completely independent (a horizontal line) of the sequencing depth, only very highly expressed genes still show some dependence; the authors claim that this is likely due to very few genes of such expression being present in the data and the regularization process thus becomes less robust.\n  There\u0026rsquo;s an unequal variance contribution in the two alternative approaches. One example that the authors highlight is how cells with low UMI counts have a disproportional influence on the variance among high-abundance genes in the log-normalization method. Something that might dampen the contribution from other gene groups. In the regularization based method, we see a very - almost surprisingly - homogeneous distribution of the variance.\n  The authors also, which I appreciate, show how the normalization can impact the downstream results as well. The most striking example - in my opinion - is when they compare outcomes of a DE (Differential Expression) analysis. They construct a simple but clever experiment, where they first grabbed all CD14+ monocytes (5551 cells) from the PBMC data set they use throughout the paper. They then split this set into two equally-sized (and mutually exhaustive) subsets, followed by downsampling of the UMIs in one of the subsets, giving them $50\\%$ of their original UMI counts but with the same distribution as before across the genes. They then subjected the two groups to a t-test in order to find DE genes. Obviously, since these are cells from the same population, we expect zero DE genes if our data is properly normalized. As you can see in the figure below (modified Figure 6E), this was not quite the case for the log-normalized data:\nRather than finding zero DE genes, using log-normalization gave us $2331$ of them. The regularized negative binomial regression wasn\u0026rsquo;t perfect either, but it gave us $11$ false positives, about $0.5\\%$ of $2331$. The authors also point out that these $11$ genes are all highly expressed genes, where we know the regularization process had some issues. They also looked into masking of true DE genes between populations of different sequencing depths and showed how log-normalization can give some weird results in these cases (incorrect assignment of DE genes) while their approach seemed to give the expected results.\nThe paper contains some other interesting results, but what I\u0026rsquo;ve presented above captures the \u0026ldquo;geist\u0026rdquo; of their method and is - at least to me - convincing enough. Thus, I\u0026rsquo;d like to focus a bit on their implementation.\nFirst I should say that the claim (in the abstract) that their procedure \u0026ldquo;omits the need for heuristic steps including psedocount addition or log-transformation\u0026rdquo;, might be a bit of a stretch. Indeed, they don\u0026rsquo;t use a pseudocount in a log-transformation, but they use a variant of the geometric mean (more robust than the arithmetic mean to outliers) when computing gene average expression:\n$$\\exp(\\frac{1}{|C|}\\sum_{c\\in C} \\log(x_{gc}+\\varepsilon)) - \\varepsilon $$\nwhere $\\varepsilon$ is \u0026ldquo;a small fixed value to avoid $\\:\\log(0)$\u0026rdquo;, which they set to $\\varepsilon = 1$ after trying several values in the range $[0.0001,1]$. In my ears this sounds an awful lot like a pseudocount, but again, minor detail.\nThe most time-consuming step, as expected, is to fit a GLM to each gene (before the kernel regression). As the whole idea with the smoothing is to learn a mapping between average expression and parameter value, they reason that it should be sufficient to use a subset of the genes for this procedure (I agree). To properly sample the range of gene means they discard uniform sampling in favor of a categorical sampling with each gene\u0026rsquo;s probability of being sampled defined as:\n$$ p_g \\propto \\frac{1}{\\log_{10}(\\bar{x}_g)}$$\nWhere $\\bar{x}_{g}$ is gene $g$\u0026rsquo;s average expression. Assessing different values, they noted that using a subset of $\\sim2000$ genes gave near identical results to the full gene set, hence this is set to the default value.\nThey also evaluated different approaches to estimate the gene specific parameters, and settled on a choice where they: assume a Poisson error distribution to estimate $(\\beta_{0g},\\beta_{1g})$, from which they can compute the mean $\\mu_g$. The mean is then kept fixed in order to estimate $\\theta_g$ using maximum likelihood. This is, due to the Poisson being a univariate distribution, much faster than using a two parameter NB error model in the GLM.\nThey also mention how their normalization approach can be extended to include other covariates than sequencing depth. When using these other covariates, two rounds of regression will be performed: one where they only use sequencing depth as a covariate, followed by one round where they inlcude all covariates but keep the sequencing depth parameters fixed. This is because the other covariates can\u0026rsquo;t be expected to share information across genes, hence no regularization can be performed with them.\nTo summarize, this paper presents a fast and interpretable way of normalizing data that is built on a statistical framework using a regularized negative binomial GLM. They convincingly show how raw UMI and log-normalization is confounded by sequencing depth, as well as our need to regularize the fitted models to get robust parameter estimates. I truly enjoyed reading this work, it\u0026rsquo;s clearly and concise with good experiments to back-up their claims.\n","date":1612093651,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612093651,"objectID":"9b38357fec5ff5239574ef87dbcb4463","permalink":"/post/2021-01-31-nbreg/","publishdate":"2021-01-31T12:47:31+01:00","relpermalink":"/post/2021-01-31-nbreg/","section":"post","summary":"For this bi-weekly paper review, my choice fell upon a paper that I\u0026rsquo;ve skimmed through multiple times before, but never gave the proper read-through it deserves. It was a definite given when I started to curate a list of papers that I wanted to include in this series of reviews.","tags":[],"title":"Paper Review | Normalization and variance stabilization of single-cell RNA-seq data using regularized negative binomial regression","type":"post"},{"authors":[],"categories":[],"content":"","date":1611859935,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611859935,"objectID":"76fc950acd38ac45ef7c62749c6e5362","permalink":"/publication/test-001/","publishdate":"2021-01-28T19:52:15+01:00","relpermalink":"/publication/test-001/","section":"publication","summary":"","tags":[],"title":"Test 001","type":"publication"},{"authors":[],"categories":null,"content":"Today I had the privilege to act as one of the teachers at the NBIS workshop \u0026ldquo;Single cell RNA-seq analysis workshop\u0026rdquo;, where I gave a lecture on spatial transcriptomics. The purpose was to give a broad overview of both the expermental and computational techniques that are available for generation and analysis of spatial transcriptomics data.\nBig thanks to Åsa and Paulo for organizing the course and inviting me, and assembling a set of really got exercises for the students to work with.\n","date":1611835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611835200,"objectID":"c897e3ce314977407b871035c11c0013","permalink":"/talk/2021-01-28-nbis/","publishdate":"2021-01-28T15:30:00+01:00","relpermalink":"/talk/2021-01-28-nbis/","section":"talk","summary":"Today I had the privilege to act as one of the teachers at the NBIS workshop \u0026ldquo;Single cell RNA-seq analysis workshop\u0026rdquo;, where I gave a lecture on spatial transcriptomics. The purpose was to give a broad overview of both the expermental and computational techniques that are available for generation and analysis of spatial transcriptomics data.","tags":[],"title":"An Orientation in the Spatial Transcriptomics landscsape","type":"talk"},{"authors":["Alma Anderson"],"categories":["reviews"],"content":"For quite some while now, I\u0026rsquo;ve carried a slight sensation of guilt within me. Guilt for being so caught up in my own projects and activities that I\u0026rsquo;ve come to neglect one of the most essential duties as a researcher: to survey, read, and contemplate upon new (and old) material that have been published. It\u0026rsquo;s so easy to put that interesting paper aside in order to finish a project deadline, as no immediate gains are observed from reading through the former in contrast to completing the latter. Once a habit, this behavior efficiently imprints the idea that \u0026ldquo;making\u0026rdquo; is more important than learning; this might give a false sense of efficiency at first, but (I believe) has a severe negative impact on the quality and creativity of one\u0026rsquo;s work in the long run. How am one supposed to grow and expand the knowledge sphere if one never travels beyond its borders? Hence, I\u0026rsquo;ve somehow managed to conjure a vision of myself being a more active reader gatherer of information this year by committing to the task of doing bi-weekly paper reviews, starting today. I should say here, that these reviews will mainly be focused on summarizing the material presented, rather than evaluating them; but of course, if I have opinions or comments which I deem relevant, these will be shared as well. My hope is to continue this at least throughout the year, to then evaluate and reassess whether its an endeavor worth continuing. Alas, let\u0026rsquo;s get started.\n Paper Title : Toward a Common Coordinate Framework for the Human Body\nAuthors : Jennifer E. Rood, Tim Stuart, Shila Ghazanfar, Tommaso Biancalani, Eyal Fisher, Andrew Butler, Anna Hupalowska, Leslie Gaffney, William Mauck, Gökçen Eraslan, John C. Marioni, Aviv Regev, Rahul Satija\nPublished : 12-12-2019\ndoi : https://doi.org/10.1016/j.cell.2019.11.019\nReview : In ambitious projects like the Human Cell Atlas we often rely on joint efforts from multiple labs, as well as collection of data across multiple individuals, ages, phenotypes, both sexes and plenty of other features. This strategy is very apt for producing large and diverse datasets; but unless we somehow harmonize and integrate all of the data into a common framework it will be nothing more than a big mess that is ought to bring more headaches than joy. The authors of this paper discuss these issues in terms of a common coordinate framework which they - very neatly - define as a reference map that can : \u0026ldquo;[..] assign a reproducible address to every location\u0026rdquo;. CCFs can be constructed at different scales (macro, meso, micro and fine are used as examples here), but its purpose always remains the same, to relate regions in independent samples in a shared context.\nPersonally, I think it\u0026rsquo;s easiest to motivate the need of a CCF through an analogy. Imagine you have a set of portraits, painted by different artists. Some paintings depict the same individual, but not all of them. An example of this scenario is illustrated below:\nNow, if we wanted to assess how skilled each painter was at recreating certain facial features (e.g., eyes or noses), we must somehow identify these different parts in each of the pictures before we could speak of comparing them. However, it\u0026rsquo;s insufficient to only compare the images pixel-by-pixel as we have several different motifs. Somehow, we need to use the common features of a face, to identify the unique elements of each artist. Similarly, when building an atlas of the human body, organs or sub-region of an organ using data from multiple sources, we need to know where each piece fits into the larger picture. A CCF would further enable a very precise exploration of variation across individuals, and is obviously a fundamental tool in any quest to study heterogeneity.\nIn the paper they discuss some of the challenges associated with the creation of a CCF, and consider the inherent anatomical diversity across individuals as perhaps the most prominent one. They also bring up a good point about how consistent annotation (used to assemble the CCF) is not as straightforward of a task as it might seem, since biological compartments and structures do not always have well-defined boundaries. Another challenge in devising methods to construct CCFs arise from the fact that a sample\u0026rsquo;s character by and large dictates how appropriate a certain method is. To illustrate this, two extremes are given as examples: (1) when anatomical regions are highly similar across all samples (e.g., early stages of embryogenesis), and (2) when cells are (seemingly) randomly organized (for example in a tumor). Of course, these two extremes represent end-points on a continuum, and most samples place somewhere in-between the two. For samples more like (1) leveraging the consistency and a priori knowledge is preferable, while for samples like (2) one would instead do better by employing data-driven methods where the locations are learnt from the data. Of course, in both scenarios it is appealing to envision a form of Bayesian approach, where we update our beliefs regarding a sample\u0026rsquo;s location as more data is gathered.\nDifferent variants of CCFs exist, three broad classes are given in the paper: using Anatomical Plane Coordinates, Landmark based construction and complex non-standard approaches. The first (anatomical plane coordinates) more or less aligns samples by registration to a reference (to account for inter-specimen variation). It uses a standard coordinate axis as the reference point to which distances are measured. One huge benefit is that distances between objects in this space, represent true distances. The landmark based approach is more or less an extension of the anatomical plane coordinates, but where one uses known landmarks (e.g., a vein bifurcation or a certain axon bundle) rather than anatomical planes. These landmarks allow one to anchor points in different samples to a reference. Having identified the landmarks, one may then apply a linear transformation to map a query image to the reference. For both the aforementioned strategies, the use of a reference is essential - the template itself may however be updated iteratively as the process progress. The complex non-standard approaches are used when there\u0026rsquo;s a lack of anatomical structure or when no clear landmarks can be identified. In complex approaches, local non-linear (in contrast to the other methods) warping is applied to account for large anatomical variation.\nNow of course, as you might have noted, both of the two first methods relies on a reference template. Hence the question of how such a template is obtained, to which the authors answers that it depends largely on the sample. For highly stereotypical samples, a successful approach has been to use an iterative process, starting with a seeding set to which the whole dataset is aligned, a new reference is then extracted and used as a seed in the next iteration; a procedure repeated until convergence. To overcome the inter-individual variability in samples with \u0026ldquo;similar inter-individual organ structure but differences in cell type location and organ dimensions\u0026rdquo; a certain degree of supervision was added to the procedure; by manual annotation of landmarks etc. Semi-supervised processes are also predicted to be of great use in the construction of a human atlas. For highly non-stereotypical samples, there were no good strategies for template construction at the date of their writing, and they mention how this will have to be remedied by new methodological developments.\nInterestingly the authors highlight a fourth orthogonal, and fairly unconventional, approach that discards the use of a pre-defined coordinate system, landmarks and templates, in favor of learning it from the data itself. Multiple examples of methods for spatial reconstruction (see Seurat v3 and novoSpaRc) can be seen as testimonies to spatial information being contained within the expression profiles of cells. To me, this is very similar to the SLAM problem in robot localization, where one tries to create the map while also finding ones current position within it.\nThey also discuss how, once a CCF is established, samples of the same and different modalities may be mapped to it. If we are operating with data of the same modality, the process should be nothing but straightforward. We simply use the same strategy as when creating the CCF itself. For other cross-modality-mapping the case is slightly different, and integration will only be possible if the assumption that both modalities shares a latent representation (e.g., chromatin accessibility) holds.\nNow, one idea that the authors introduce, and which according to them is novel (I can find nothing that would contradict the claim) is that given all these challenges and how much influence a sample\u0026rsquo;s character has on the choice of method, is to dismiss the objective of creating a single CCF. Instead, multiple CCFs should be created in a hierarchical (by scale) fashion, which would allow both horizontal and vertical movements across the atlas, but where each CCF is adapted to best fit the data. This would represent a sort of \u0026ldquo;atlas of atlases\u0026rdquo; as they express it. To me, this makes perfect sense, although it would have been interesting to see some more discussion of how the vertical movements (between scales) were envisioned.\nTheir conclusions are brief, and so shall mine be. They make a strong argument for the value of CCFs, and why they deserve our attention. Inter-individual variability is the biggest hurdle to tackle, but there are strategies for it, though highly dependent on the data; this is not a \u0026ldquo;one-size fits all\u0026rdquo; type of problem. When the data is complex and we can\u0026rsquo;t establish natural anatomical templates or find good landmarks, perhaps it\u0026rsquo;s better to let the data dictate its own reference, using approaches inspired from methods of spatial reconstruction. All in all, CCFs brings structure to a very chaotic and seemingly unstructured space - and they are imperative to the process of creating a unified atlas of the human body.\n","date":1611064897,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611064897,"objectID":"38dba622c205a2a69ebb198c9cc0e503","permalink":"/post/2021-01-19-ccf-rev/","publishdate":"2021-01-19T15:01:37+01:00","relpermalink":"/post/2021-01-19-ccf-rev/","section":"post","summary":"For quite some while now, I\u0026rsquo;ve carried a slight sensation of guilt within me. Guilt for being so caught up in my own projects and activities that I\u0026rsquo;ve come to neglect one of the most essential duties as a researcher: to survey, read, and contemplate upon new (and old) material that have been published.","tags":["review","CCF","common coordinate framework"],"title":"Paper Review | Toward a Common Coordinate Framework for the Human Body","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;m thrilled to announce that a collaboration with my good friend and colleague, Franziska Hildebrandt, now has matured to the state of being ready for journal submission, which also coincides with a pre-print upload to bioRxiv.\nThe pre-print is titled \u0026ldquo;Spatial Transcriptomics to define transcriptional patterns of zonation and structural components in the liver\u0026rdquo;, and focus on the application of ST2K (similar to the old legacy ST1K and the newer Visium platform). It is the first time that ST (Spatial Transcriptomics) has been applied to the mouse liver, and aims to show how the technique enables the liver to be explored from new, different and illuminating perspective. A lot of standard analysis, such as clustering and identification of differential genes (DEGs) are presented in the paper and show affirmative results, attesting to the (experimental) method\u0026rsquo;s ability to delineate regions with various expression profiles (often overlapping well with morphological structures).\nMy contribution to this paper was nevertheless slightly different and consisted mainly of two parts:\n Construction of a framework to model feature signals as a function of the distance to a given object. Development of a classifier to predict vein type (a binary classification task) based on the expression profiles surrounding a certain genes.  Both concepts are excellent examples of analyses that are exclusive to spatial data, i.e., similar information cannot be obtained from single cell experiments; something I\u0026rsquo;m keen to highlight, as it is often the case that one fails to make proper use of the \u0026ldquo;spatial\u0026rdquo; part in spatial transcriptomics data, and rather treat it as single cell data, to then visualize the results spatially. Both implementations are very simple, but I have not seen similar analyses in ST or Visium data yet, of course that could be due to me not having surveyed the field thoroughly enough, but still. Allow me to elaborate some on these two concepts below.\nFeature by distance In the liver tissue one can broadly classify vein structures as either portal or central based on their position and connections. Previous studies have shown that these structures are associated with slightly different expression profiles, as different celltypes (periportal respective pericentral hepatocytes) are present in them.\nWhat we did in this study was to first \u0026ldquo;measure\u0026rdquo; the distance of each spot (spatial capture location) to the nearest vein (of respective type), and then for each gene create a set of distance-expression tuples that could be visualized in a scatter plot. While these values are discrete, once a curve smooting techniques is applied we can easily think of the resulting curve as a function that maps distance to gene expression. By doing so one could really see how the associated marker genes (to each vein) had a high expression near their respective vein, and how this decreased as distance increased (inverse relationship). You can see some examples of these trends in the plots below, top row displays portal vein marker genes, bottom central:\nClassification of vein types by expression Now, while pathologists and experts in the field usually are able to annotate each vein type by visual inspection (this is how we obtained the annotations used for the feature by distance plots), there are sometimes ambiguities and limited access to such individuals. However, the expression profiles of portal and central veins should be the similar for all veins of the same type, independent of whether identity is easy to deduce from morphology or not. Hence, by creating neighborhood expression profiles (NEPs) surrounding each vein, and training a classifier (actually logistic regression) using NEPs from veins with known identities; we were able to predict vein type of ambiguous veins. During cross-validation (leave one sample out) the model also performed very well with higher than 80% accuracy.\nTo read more about the study, check out the manuscript here, and the Python package we released (hepaquery) to perform similar analyses with any type of spatial transcriptomics data at the github page.\nI\u0026rsquo;m very happy to have worked with a nice team with complementary biology and computational skills, this really resulted in a neat manuscript; also a big kudos to the first author Franziska, who pushed very hard in the end to finalize this project.\n","date":1610449159,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610449159,"objectID":"fd7e03d9d0f710dfa8d11e7f9a57ef2d","permalink":"/post/2021-01-12-liver-biorxiv/","publishdate":"2021-01-12T11:59:19+01:00","relpermalink":"/post/2021-01-12-liver-biorxiv/","section":"post","summary":"I\u0026rsquo;m thrilled to announce that a collaboration with my good friend and colleague, Franziska Hildebrandt, now has matured to the state of being ready for journal submission, which also coincides with a pre-print upload to bioRxiv.","tags":[],"title":"Manuscript | Spatial Transcriptomics to define transcriptional patterns of zonation and structural components in the liver","type":"post"},{"authors":["Alma Anderson"],"categories":[],"content":"I\u0026rsquo;ve always found it appealing, or rather enticing, to study biological systems from a more formal and mathematical perspective; with the main objective to somehow be able to describe them with equations rather than words. Equations are exact, definite, universal, but foremost, they are predictive. The alluring property of mathematical models is that they do not only provide a description of our system, but they are also responsive, we may ask \u0026ldquo;if I replace parameter X with parameter Y, what will my system look like?\u0026quot;.\nOf course, our models - however beautiful or carefully constructed - are nothing but just that, models. And as the well known apohorism states, \u0026ldquo;all models are wrong\u0026rdquo;. While this statement is true, it is far from detrimental. Sometimes, being wrong but within a certain marginal of error to the truth is just about good enough. When driving, none of us knows the exact distance to the other cars, or how much a push on the gas will accelerate our car, still (most of the time) we manage to zig-zag through the densely packed lanes without crashing into each other - all from the simple incorrect model we constructed in our minds. So, yes - models are wrong, but they are most certainly useful and tremendously valuable in our quest to understand and analyze biological systems.\nIt was with this interest - and a requirement for PhD students at KTH to collect 60 ECTS credits - that I enrolled for a course in Applied Estimation. In short, the course focus on how to estimate states or parameters from noisy measurements such as empirical data. Albeit designed for people working with robotics, the concepts are indubitably applicable to biology - where noise is the rule rather than the exception.\nAs a part of the course, we had to devise our own project (where one estimation method was to used) and implement it in code. My choice fell upon a subject (and method) that I have little previous experience with: automated cell tracking in brightfield images, using GM-PHD (Gaussian Mixture Probability Hypothesis Density) filters. The theory is very interesting, and I was particularly intrigued by the concept of Random Finite Sets, here used to track multiple objects simultaneously. The method shares many similarities with the Kalman Filter, but rather than propagating a single state, the whole set is propagated in time. Easily explained, one might think of it as propagating a Gaussian Mixture in time, where each component represents a peak or cell. The original publication by Vo and Ma outlines the details of the GM-PHD filter better than I could ever attempt to, and I would refer anyone with an interest to have a close look at it.\nAs for the final product, I implemented the algorithm proposed by Vo and Ma (Table I-III) in python, with an easy to use CLI; allowing anyone who might want to try it out to do so fairly (I hope) seamlessly. Instructions and examples of usage can be found at the github page. The animated image above is the result of applying this implementation to a set of \u0026ldquo;HeLa cells stably expressing H2b-GFP\u0026rdquo;, downloaded from celltrackingchallenge.net. The implementation is far from perfect, and much work remains to even have a chance of competing with the more sophisticated alternatives. Still, for a couple of days of work, the performance is not too bad - and it was a fun experience implementing it.\nThe course has prompted a lot of new ideas and really allowed me to see some old problems from a completely different lens. For example, I believe a lot of the ideas found in SLAM (Simultaneous Localization and Mapping) could be used when working with questions related to trajectory inferenc, where we partly want to assess the developmental landscape but also map a path through it. It\u0026rsquo;s always a very rewarding experience to enter a completely new field, where people come from a very different background - in my opinion this intentional \u0026ldquo;push\u0026rdquo; out of the comfort zone really has a tendency to spark great exciting ideas, however uncofortable it feels at first.\n","date":1610097951,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610097951,"objectID":"bd2c4610723e1a9d61a3ef3498fcc267","permalink":"/post/2021-01-08-celltracker/","publishdate":"2021-01-08T10:25:51+01:00","relpermalink":"/post/2021-01-08-celltracker/","section":"post","summary":"I\u0026rsquo;ve always found it appealing, or rather enticing, to study biological systems from a more formal and mathematical perspective; with the main objective to somehow be able to describe them with equations rather than words.","tags":[],"title":"Models and automated cell tracking","type":"post"},{"authors":[],"categories":null,"content":"","date":1605884100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605884100,"objectID":"5ccf3a9fac565207aef25af7914f7876","permalink":"/talk/2020-11-20-ugm/","publishdate":"2020-11-20T16:25:00+01:00","relpermalink":"/talk/2020-11-20-ugm/","section":"talk","summary":"","tags":[],"title":"Exploring the cell type topography by integration of spatial and single cell data","type":"talk"},{"authors":[],"categories":null,"content":"","date":1603359000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603359000,"objectID":"ff44752e2eefe605bb3d2e05721fef75","permalink":"/talk/2020-10-22-10xrs/","publishdate":"2020-10-22T11:00:00+01:00","relpermalink":"/talk/2020-10-22-10xrs/","section":"talk","summary":"","tags":[],"title":"Exploring the cell type topography by integration of spatial and single cell data","type":"talk"},{"authors":["Alma Anderson"],"categories":[],"content":"Delighted to say that our manuscript \u0026ldquo;Single-cell and spatial transcriptomics enables probabilistic inference of cell type topography\u0026rdquo; is now published in Communications Biology and thus out there for everyone to read in its final form. This is the paper that describes the theoretical underpinnings for stereoscope, which allows you to spatial map cell types found in single cell data onto spatial transcriptomics data. There is also a nice bit of discussion as to whether spatial transcriptomics data (ST/Visium) can be properly modeled using a negative binomial distribution (which we use) found in the supplementary (spoiler, it seems like it). Have a look at it if you are interested!\n manuscript link : HERE stereoscope link : HERE  P.S: I\u0026rsquo;m also planning to update the code base in the future, providing an API for scanpy, but also add some features like subsampling and gene selection to the standard modules; but that is for later.\n","date":1602237904,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602237904,"objectID":"94664ec0002f5a54210a1bf6745bbbe7","permalink":"/post/stereoscope-001/initial-post/","publishdate":"2020-10-09T11:05:04+01:00","relpermalink":"/post/stereoscope-001/initial-post/","section":"post","summary":"Delighted to say that our manuscript \u0026ldquo;Single-cell and spatial transcriptomics enables probabilistic inference of cell type topography\u0026rdquo; is now published in Communications Biology and thus out there for everyone to read in its final form.","tags":["publications","single cell mapping","stereoscope"],"title":"Publication | Spatial Single Cell Mapping","type":"post"},{"authors":["Alma Anderson"],"categories":[],"content":"We\u0026rsquo;ve put out a new manuscript on bioRxiv, where the expression landscape of HER2-positive breast cancer tumors is studied from a spatial perspective. This study contains several examples of how spatial data may be analyzed and what information beyond the obvious - like the spatial distribution of a certain gene - that can be mined from it. My hope is that anyone looking for ideas regarding how to analyze their own spatial data find this useful and relevant, irregardles of their interest in breast cancer. In the study we cluster spots by gene expression, but also take this one step further, using the clusters to find core signatures of immune and tumor cell populations within our samples. We also use stereoscope to map single cell data down onto our tissue sections, information which then is used to indentify potential TLS-sites as well as to study patterns of cell type co-localization. All the scripts and a neat app for visualization of the results can be found here.\n","date":1594807504,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594821904,"objectID":"2201b458b56f3a4235bf472b7c194dfd","permalink":"/post/her2-001/initial-post/","publishdate":"2020-07-15T11:05:04+01:00","relpermalink":"/post/her2-001/initial-post/","section":"post","summary":"We\u0026rsquo;ve put out a new manuscript on bioRxiv, where the expression landscape of HER2-positive breast cancer tumors is studied from a spatial perspective. This study contains several examples of how spatial data may be analyzed and what information beyond the obvious - like the spatial distribution of a certain gene - that can be mined from it.","tags":[],"title":"Manuscript | Spatial Exploration of Her2 Breast Cancer","type":"post"},{"authors":["Alma Anderson"],"categories":[],"content":"Browsing through my daily feed, I stumbled upon this neat resource which lists (and links to) fundamental ideas that have shaped the field of systems biology. To me, many of these publications represent some of the earliest attempts to reduce complex biological systems and processes into something simpler, more comprehensible, by viewing them through the lense of mathematics; truly groundbreaking efforts that paved the way for the current trends of ML/AI in biology. I\u0026rsquo;d strongly recommend anyone with an interest in the field to have a look at it; something that contains work from both Alan Turing and John von Neumann can\u0026rsquo;t be anything but good!\n","date":1593594304,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593594304,"objectID":"3ecfac006474e62c5ea80b86b638f81e","permalink":"/post/synt-bio-001/initial-post/","publishdate":"2020-07-01T10:05:04+01:00","relpermalink":"/post/synt-bio-001/initial-post/","section":"post","summary":"Browsing through my daily feed, I stumbled upon this neat resource which lists (and links to) fundamental ideas that have shaped the field of systems biology. To me, many of these publications represent some of the earliest attempts to reduce complex biological systems and processes into something simpler, more comprehensible, by viewing them through the lense of mathematics; truly groundbreaking efforts that paved the way for the current trends of ML/AI in biology.","tags":[],"title":"Resource | Formal systems in Biology","type":"post"},{"authors":[],"categories":null,"content":"","date":1591189200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591189200,"objectID":"8e68e4ef91d573ebdff6a6a5aba7e08e","permalink":"/talk/2020-06-03-symp/","publishdate":"2020-06-03T16:00:00+01:00","relpermalink":"/talk/2020-06-03-symp/","section":"talk","summary":"","tags":[],"title":"Exploring the cell type topography by integration of spatial and single cell data","type":"talk"},{"authors":[],"categories":null,"content":"","date":1590768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590768000,"objectID":"330e1e85b390d61cfdda5cf579d9f4e9","permalink":"/talk/2020-05-29-scadv/","publishdate":"2020-05-29T17:00:00+01:00","relpermalink":"/talk/2020-05-29-scadv/","section":"talk","summary":"","tags":[],"title":"Computational Analysis of Spatial Transcriptomics Data","type":"talk"},{"authors":[],"categories":[],"content":"I recently compiled some exercises focusing on analysis of spatial data for a course (hosted by SIB). The material spans from very basic analysis (visualization and clustering) to inference of spatial cell type distributions by integration of single cell and spatial data. If you are excited about spatial transcriptomics and what type of analysis you can apply to this type of data, check out the material here . It\u0026rsquo;s all written using standard python libraries for maximal transparency, allowing you to follow each and every step.\n","date":1590761104,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590761104,"objectID":"81bf1cb397776fe1e1fb0378851b2df6","permalink":"/post/init/initial-post/","publishdate":"2020-05-29T15:05:04+01:00","relpermalink":"/post/init/initial-post/","section":"post","summary":"I recently compiled some exercises focusing on analysis of spatial data for a course (hosted by SIB). The material spans from very basic analysis (visualization and clustering) to inference of spatial cell type distributions by integration of single cell and spatial data.","tags":[],"title":"Exercises | Spatial Transcriptomics Data Analysis","type":"post"},{"authors":[],"categories":null,"content":"","date":1587553783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587553783,"objectID":"61f427c79d2e31f65b5acc1f1fe71e3d","permalink":"/talk/2020-04-22-emea/","publishdate":"2020-04-22T12:09:43+01:00","relpermalink":"/talk/2020-04-22-emea/","section":"talk","summary":"","tags":[],"title":"Exploring the cell type topography by integration of single cell and spatial transcriptomics data","type":"talk"}]