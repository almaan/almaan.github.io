[{"authors":null,"categories":null,"content":"I\u0026rsquo;m a PhD student in the Lundeberg Lab, situated at SciLifeLab (Stockholm, Sweden) focusing on computational method development. I\u0026rsquo;ve always been fascinated with how mathematical models can be used to explain much of the richness found in complex systems (biological and non-biological) but also to enhance our understanding of their characteristics and behaviour; a fascination that - for the better or worse - tend to influence my research quite substantially. My second big passion is running, the farther the better, whether it\u0026rsquo;s for transportation or pure pleasure - nothing can quite compare to it.\n","date":1612093651,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1612093651,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/alma-anderson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alma-anderson/","section":"authors","summary":"I\u0026rsquo;m a PhD student in the Lundeberg Lab, situated at SciLifeLab (Stockholm, Sweden) focusing on computational method development. I\u0026rsquo;ve always been fascinated with how mathematical models can be used to explain much of the richness found in complex systems (biological and non-biological) but also to enhance our understanding of their characteristics and behaviour; a fascination that - for the better or worse - tend to influence my research quite substantially.","tags":null,"title":"Alma Anderson","type":"authors"},{"authors":["Alma Anderson"],"categories":[],"content":"For this bi-weekly paper review, my choice fell upon a paper that I\u0026rsquo;ve skimmed through multiple times before, but never gave the proper read-through it deserves. It was a definite given when I started to curate a list of papers that I wanted to include in this series of reviews. Not only do I find it to be an elegant and well-composed paper, but it has had nothing but an /imperative/ influence on the RNA-seq based analysis these last years. In the paper, they present a model for normalization (and variance stabilization) of single-cell RNA-seq data, which now has become widely adopted within the community. Almost every paper not focusing on method development that I\u0026rsquo;ve stumbled across, use the \u0026ldquo;sctransform\u0026rdquo; package or derivatives of it to normalize scRNA-seq data. Thus, without further ado let us begin.\n Paper Title : Normalization and variance stabilization of single-cell RNA-seq data using regularized negative binomial regression\nAuthors : Christoph Hafemeister and Rahul Satija\nPublished 23 December 2019\ndoi : https://doi.org/10.1186/s13059-019-1874-1\nReview : Anyone who has ever worked with single cell RNA-seq data knows that before any \u0026ldquo;flashy and cool\u0026rdquo; results can be extracted, we need to apply some pre-processing steps to our data. Aside from filtering for low quality observations or irrelevant features, the very first step - and perhaps most important - tends to be normalization of the data. Normalization is a dead given in any analysis workflow, but let us just take one step back and ask why? Why do we care about normalization, why can\u0026rsquo;t we just \u0026ldquo;jump right at it\u0026rdquo;?\nProbing our data with the right tools will allow us to identify cell types and cell states as well as cell-to-cell variations. However if we aren\u0026rsquo;t careful technical factors might confound these results, making us come to incorrect and false conclusions regarding the patterns in our data. One technical factor that has a huge influence on the result of our downstream analysis is the sequencing depth. For example, imagine that we have two cell types A and B, where cells of A in general have higher transcriptional activity than those of B. Now if we were to compare the (raw and unprocessed) expression of a houskeeping gene between cells from the two types, it would be listed as upregulated in A, even though we expect all housekeeping genes to be of approximately equal relative expression across cell types. This is of course undesireable, and we therefore attempt to normalize the data, in order to remove any technical variation while preferably maintaining biological differences.\nThe authors define two \u0026ldquo;objectives\u0026rdquo; or criteria, that should hold for a single-cell data set to be considered sucessfully normalized:\n Normalized expression should be uncorrelated with sequencing depth of cells. Variance of a normalized gene (across cells) should not be affected by gene abundance or sequencing depth.  In contrast to the two previously existing paradigms of using either cell-specific scaling factors or probabilistic approaches they consider their approach as more of a statistical approach, distinct from the others. I would perhaps say that their strategy could fit into to the set of probabilistic approaches, but that\u0026rsquo;s a minor detail.\nThe method they present builds on a generalized linear model (GLM) where they use a constrained negative binomial error model, with sequencing depth as a covariate, and a log-link function for the mean.\nAfter evaluating different error models such as: an unconstrained Poisson, a Negative Binomial and a Zero Inflated Negative Binomial (ZINB), they noticed that the Poisson was not flexible enough, while the unconstrained NB and ZINB models were prone to overfitting. Therefore, as is common in several bulk normalization methods, they decided to regularize the model parameters by pooling information from genes with similar mean expression values, resulting in a regularized negative binomial regression. The constrained part is one of the model\u0026rsquo;s key features, the other one being to include the sequencing depth as a covariate. We will soon come back to the regularization, but let us first just briefly look at their GLM model:\n$$ \\log(E[x_{gc}]) = \\log(\\mu_{gc}) = \\beta_{0g} + \\beta_{1g} \\log_{ 10 }(m_c) $$\nWhere $x_{gc}$ is the expression of gene $g$ in cell $c$, and $m_{c}$ is the library size used as a proxy for sequencing depth.\nIn short, this means that the authors make the implicit assumption that most genes are not differentially expressed; genes have a common baseline ($\\beta_{0g}$) and differences across cells will mainly be due to their varying library size (influence controlled by $\\beta_{1g}$).\nThe negative binomial has one more parameter that we need to pay attention to, the dispersion parameter $\\theta$ which gives us the variance of our data. That is, if:\n$$ x \\sim NB(\\mu,\\theta)$$\nthen\n$$E[x] = \\mu, \\quad Var[x] = \\mu + \\frac{\\mu^2}{\\theta}$$\nInterestingly, the authors convincingly show that if we were to learn one set of parameters for each gene, these will be severely overfitted. What really made me appreciate the prevalance of this overfitting, was the results they present in Figure 2 (included below).\nIn the top row they show the estimated parameter values plotted as a function of gene mean; in a good model we would expect to have similar parameter values for genes with similar mean values. In the bottom row, they fit their GLM model to a random subsets of the data and assessed the variance of the parameter values (in a bootstrap fashion), showing how the variance (red dots) was very high, especially for low and medium expressed genes.\nTo remedy this issue of overfitting, they regularize their model, though not using the regularization terms that perhaps most of our familiar with like a $l_1$ or $l_2$ penalty. Instead they pool information from several genes to make more robust estimates. They first fit one GLM to each gene, but then proceed to apply kernel regression to the resulting parameter estimates. The intent being to learn regularized parameters that depend on a gene\u0026rsquo;s average expression. The blue dots in panel B of the figure above show the variance after using this regularized approach; as you can see the variance in parameter estimates is significantly reduced.\nThough, we still have some work left to do before our data is normalized. Having obtained the set of regularized parameters, one applies this regression model to the data in order to compute the residuals ($r_{gc}$).\n$$ r_{gc} = x_{gc} - \\mu_{gc}, \\quad \\mu_{gc} = \\exp[\\beta_{0g}+ \\beta_{1g}\\log_{10}(m_c)] $$\nThe residuals represent the difference between the response estimated mean and the observed expression value. But as one might remember, for the negative binomial, the variance can differ between genes even though they have the same mean ($\\mu$), an effect of the dispersion parameter $\\theta$. Thus, we will not use the raw residuals, but what is known as the Pearson residuals ($z_{gc}$), which account for the gene-specific variation term:\n$$ z_{gc} = \\frac{r_{gc}}{\\sigma_{gc}}, \\quad \\sigma_{gc} = \\sqrt{\\mu_{gc} + \\frac{\\mu_{gc}^2}{\\theta_g}} $$\nThis transformation presents the residuals in units of standard deviations, correcting for the specific gene variance. Implementationwise they also clip their $z_{gc}$ values to be less than $\\sqrt{N}$ (where $N = $ number of cells), in order to \u0026ldquo;reduce the impact of extreme outliers\u0026rdquo;. This value seems to have been determined fairly empirically.\nThe Pearson residuals are treated as normalized expression levels; the idea being that we\u0026rsquo;ve now regressed out any contribution from the sequencing depth to the observed expression levels. Using Pearson residuals has one additional benefit, being that the transformation we apply to compute them is inherently variance-stabilizing.\nThe image below is a merge between Figure 1C-E and Figure 3A-B and requires a bit of explanation in order to make sense. The curves represent the expression values as a function of cell UMI count where the genes have been divided into six groups based on their average expression in the data set. The six groups are not equally sized, but represent bins with equal width spanning the expression range. The bottom bar graphs are slightly more complex; the same six gene groups are used, and the cells are divided into five equally-sized groups with increasing average sequencing depth (group 1 having the highest value). Next, they calculated the total variance in each gene to then see how much each cell group contributed to this value. In perfectly normalized data, one would expect to see an equal contribution (here $20\\%$) between the cell groups, as gene expression should be uncorrelated with sequencing depth (criterion 1 from their \u0026ldquo;objectives\u0026rdquo;).\nComparing the use of Pearson residuals with raw UMI counts and log-normaliztion, we observe the following:\n  Using raw UMI counts, a clear correlation between sequencing depth and gene expression in all ranges of gene expression is present. Log-normalization mitigate this issue among lowly expressed genes (group 4-5), but it persists in the high and medium expressed genes. The regularized negative binomial normalization renders values near completely independent (a horizontal line) of the sequencing depth, only very highly expressed genes still show some dependence; the authors claim that this is likely due to very few genes of such expression being present in the data and the regularization process thus becomes less robust.\n  There\u0026rsquo;s an unequal variance contribution in the two alternative approaches. One example that the authors highlight is how cells with low UMI counts have a disproportional influence on the variance among high-abundance genes in the log-normalization method. Something that might dampen the contribution from other gene groups. In the regularization based method, we see a very - almost surprisingly - homogeneous distribution of the variance.\n  The authors also, which I appreciate, show how the normalization can impact the downstream results as well. The most striking example - in my opinion - is when they compare outcomes of a DE (Differential Expression) analysis. They construct a simple but clever experiment, where they first grabbed all CD14+ monocytes (5551 cells) from the PBMC data set they use throughout the paper. They then split this set into two equally-sized (and mutually exhaustive) subsets, followed by downsampling of the UMIs in one of the subsets, giving them $50\\%$ of their original UMI counts but with the same distribution as before across the genes. They then subjected the two groups to a t-test in order to find DE genes. Obviously, since these are cells from the same population, we expect zero DE genes if our data is properly normalized. As you can see in the figure below (modified Figure 6E), this was not quite the case for the log-normalized data:\nRather than finding zero DE genes, using log-normalization gave us $2331$ of them. The regularized negative binomial regression wasn\u0026rsquo;t perfect either, but it gave us $11$ false positives, about $0.5\\%$ of $2331$. The authors also point out that these $11$ genes are all highly expressed genes, where we know the regularization process had some issues. They also looked into masking of true DE genes between populations of different sequencing depths and showed how log-normalization can give some weird results in these cases (incorrect assignment of DE genes) while their approach seemed to give the expected results.\nThe paper contains some other interesting results, but what I\u0026rsquo;ve presented above captures the \u0026ldquo;geist\u0026rdquo; of their method and is - at least to me - convincing enough. Thus, I\u0026rsquo;d like to focus a bit on their implementation.\nFirst I should say that the claim (in the abstract) that their procedure \u0026ldquo;omits the need for heuristic steps including psedocount addition or log-transformation\u0026rdquo;, might be a bit of a stretch. Indeed, they don\u0026rsquo;t use a pseudocount in a log-transformation, but they use a variant of the geometric mean (more robust than the arithmetic mean to outliers) when computing gene average expression:\n$$\\exp(\\frac{1}{|C|}\\sum_{c\\in C} \\log(x_{gc}+\\varepsilon)) - \\varepsilon $$\nwhere $\\varepsilon$ is \u0026ldquo;a small fixed value to avoid $\\:\\log(0)$\u0026rdquo;, which they set to $\\varepsilon = 1$ after trying several values in the range $[0.0001,1]$. In my ears this sounds an awful lot like a pseudocount, but again, minor detail.\nThe most time-consuming step, as expected, is to fit a GLM to each gene (before the kernel regression). As the whole idea with the smoothing is to learn a mapping between average expression and parameter value, they reason that it should be sufficient to use a subset of the genes for this procedure (I agree). To properly sample the range of gene means they discard uniform sampling in favor of a categorical sampling with each gene\u0026rsquo;s probability of being sampled defined as:\n$$ p_g \\propto \\frac{1}{\\log_{10}(\\bar{x}_g)}$$\nWhere $\\bar{x}_{g}$ is gene $g$\u0026rsquo;s average expression. Assessing different values, they noted that using a subset of $\\sim2000$ genes gave near identical results to the full gene set, hence this is set to the default value.\nThey also evaluated different approaches to estimate the gene specific parameters, and settled on a choice where they: assume a Poisson error distribution to estimate $(\\beta_{0g},\\beta_{1g})$, from which they can compute the mean $\\mu_g$. The mean is then kept fixed in order to estimate $\\theta_g$ using maximum likelihood. This is, due to the Poisson being a univariate distribution, much faster than using a two parameter NB error model in the GLM.\nThey also mention how their normalization approach can be extended to include other covariates than sequencing depth. When using these other covariates, two rounds of regression will be performed: one where they only use sequencing depth as a covariate, followed by one round where they inlcude all covariates but keep the sequencing depth parameters fixed. This is because the other covariates can\u0026rsquo;t be expected to share information across genes, hence no regularization can be performed with them.\nTo summarize, this paper presents a fast and interpretable way of normalizing data that is built on a statistical framework using a regularized negative binomial GLM. They convincingly show how raw UMI and log-normalization is confounded by sequencing depth, as well as our need to regularize the fitted models to get robust parameter estimates. I truly enjoyed reading this work, it\u0026rsquo;s clearly and concise with good experiments to back-up their claims.\n","date":1612093651,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612093651,"objectID":"9b38357fec5ff5239574ef87dbcb4463","permalink":"/post/2021-01-31-nbreg/","publishdate":"2021-01-31T12:47:31+01:00","relpermalink":"/post/2021-01-31-nbreg/","section":"post","summary":"For this bi-weekly paper review, my choice fell upon a paper that I\u0026rsquo;ve skimmed through multiple times before, but never gave the proper read-through it deserves. It was a definite given when I started to curate a list of papers that I wanted to include in this series of reviews.","tags":[],"title":"Paper Review | Normalization and variance stabilization of single-cell RNA-seq data using regularized negative binomial regression","type":"post"},{"authors":[],"categories":[],"content":"","date":1611859935,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611859935,"objectID":"76fc950acd38ac45ef7c62749c6e5362","permalink":"/publication/test-001/","publishdate":"2021-01-28T19:52:15+01:00","relpermalink":"/publication/test-001/","section":"publication","summary":"","tags":[],"title":"Test 001","type":"publication"},{"authors":[],"categories":null,"content":"Today I had the privilege to act as one of the teachers at the NBIS workshop \u0026ldquo;Single cell RNA-seq analysis workshop\u0026rdquo;, where I gave a lecture on spatial transcriptomics. The purpose was to give a broad overview of both the expermental and computational techniques that are available for generation and analysis of spatial transcriptomics data.\nBig thanks to Åsa and Paulo for organizing the course and inviting me, and assembling a set of really got exercises for the students to work with.\n","date":1611835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611835200,"objectID":"c897e3ce314977407b871035c11c0013","permalink":"/talk/2021-01-28-nbis/","publishdate":"2021-01-28T15:30:00+01:00","relpermalink":"/talk/2021-01-28-nbis/","section":"talk","summary":"Today I had the privilege to act as one of the teachers at the NBIS workshop \u0026ldquo;Single cell RNA-seq analysis workshop\u0026rdquo;, where I gave a lecture on spatial transcriptomics. The purpose was to give a broad overview of both the expermental and computational techniques that are available for generation and analysis of spatial transcriptomics data.","tags":[],"title":"An Orientation in the Spatial Transcriptomics landscsape","type":"talk"},{"authors":["Alma Anderson"],"categories":["reviews"],"content":"For quite some while now, I\u0026rsquo;ve carried a slight sensation of guilt within me. Guilt for being so caught up in my own projects and activities that I\u0026rsquo;ve come to neglect one of the most essential duties as a researcher: to survey, read, and contemplate upon new (and old) material that have been published. It\u0026rsquo;s so easy to put that interesting paper aside in order to finish a project deadline, as no immediate gains are observed from reading through the former in contrast to completing the latter. Once a habit, this behavior efficiently imprints the idea that \u0026ldquo;making\u0026rdquo; is more important than learning; this might give a false sense of efficiency at first, but (I believe) has a severe negative impact on the quality and creativity of one\u0026rsquo;s work in the long run. How am one supposed to grow and expand the knowledge sphere if one never travels beyond its borders? Hence, I\u0026rsquo;ve somehow managed to conjure a vision of myself being a more active reader gatherer of information this year by committing to the task of doing bi-weekly paper reviews, starting today. I should say here, that these reviews will mainly be focused on summarizing the material presented, rather than evaluating them; but of course, if I have opinions or comments which I deem relevant, these will be shared as well. My hope is to continue this at least throughout the year, to then evaluate and reassess whether its an endeavor worth continuing. Alas, let\u0026rsquo;s get started.\n Paper Title : Toward a Common Coordinate Framework for the Human Body\nAuthors : Jennifer E. Rood, Tim Stuart, Shila Ghazanfar, Tommaso Biancalani, Eyal Fisher, Andrew Butler, Anna Hupalowska, Leslie Gaffney, William Mauck, Gökçen Eraslan, John C. Marioni, Aviv Regev, Rahul Satija\nPublished : 12-12-2019\ndoi : https://doi.org/10.1016/j.cell.2019.11.019\nReview : In ambitious projects like the Human Cell Atlas we often rely on joint efforts from multiple labs, as well as collection of data across multiple individuals, ages, phenotypes, both sexes and plenty of other features. This strategy is very apt for producing large and diverse datasets; but unless we somehow harmonize and integrate all of the data into a common framework it will be nothing more than a big mess that is ought to bring more headaches than joy. The authors of this paper discuss these issues in terms of a common coordinate framework which they - very neatly - define as a reference map that can : \u0026ldquo;[..] assign a reproducible address to every location\u0026rdquo;. CCFs can be constructed at different scales (macro, meso, micro and fine are used as examples here), but its purpose always remains the same, to relate regions in independent samples in a shared context.\nPersonally, I think it\u0026rsquo;s easiest to motivate the need of a CCF through an analogy. Imagine you have a set of portraits, painted by different artists. Some paintings depict the same individual, but not all of them. An example of this scenario is illustrated below:\nNow, if we wanted to assess how skilled each painter was at recreating certain facial features (e.g., eyes or noses), we must somehow identify these different parts in each of the pictures before we could speak of comparing them. However, it\u0026rsquo;s insufficient to only compare the images pixel-by-pixel as we have several different motifs. Somehow, we need to use the common features of a face, to identify the unique elements of each artist. Similarly, when building an atlas of the human body, organs or sub-region of an organ using data from multiple sources, we need to know where each piece fits into the larger picture. A CCF would further enable a very precise exploration of variation across individuals, and is obviously a fundamental tool in any quest to study heterogeneity.\nIn the paper they discuss some of the challenges associated with the creation of a CCF, and consider the inherent anatomical diversity across individuals as perhaps the most prominent one. They also bring up a good point about how consistent annotation (used to assemble the CCF) is not as straightforward of a task as it might seem, since biological compartments and structures do not always have well-defined boundaries. Another challenge in devising methods to construct CCFs arise from the fact that a sample\u0026rsquo;s character by and large dictates how appropriate a certain method is. To illustrate this, two extremes are given as examples: (1) when anatomical regions are highly similar across all samples (e.g., early stages of embryogenesis), and (2) when cells are (seemingly) randomly organized (for example in a tumor). Of course, these two extremes represent end-points on a continuum, and most samples place somewhere in-between the two. For samples more like (1) leveraging the consistency and a priori knowledge is preferable, while for samples like (2) one would instead do better by employing data-driven methods where the locations are learnt from the data. Of course, in both scenarios it is appealing to envision a form of Bayesian approach, where we update our beliefs regarding a sample\u0026rsquo;s location as more data is gathered.\nDifferent variants of CCFs exist, three broad classes are given in the paper: using Anatomical Plane Coordinates, Landmark based construction and complex non-standard approaches. The first (anatomical plane coordinates) more or less aligns samples by registration to a reference (to account for inter-specimen variation). It uses a standard coordinate axis as the reference point to which distances are measured. One huge benefit is that distances between objects in this space, represent true distances. The landmark based approach is more or less an extension of the anatomical plane coordinates, but where one uses known landmarks (e.g., a vein bifurcation or a certain axon bundle) rather than anatomical planes. These landmarks allow one to anchor points in different samples to a reference. Having identified the landmarks, one may then apply a linear transformation to map a query image to the reference. For both the aforementioned strategies, the use of a reference is essential - the template itself may however be updated iteratively as the process progress. The complex non-standard approaches are used when there\u0026rsquo;s a lack of anatomical structure or when no clear landmarks can be identified. In complex approaches, local non-linear (in contrast to the other methods) warping is applied to account for large anatomical variation.\nNow of course, as you might have noted, both of the two first methods relies on a reference template. Hence the question of how such a template is obtained, to which the authors answers that it depends largely on the sample. For highly stereotypical samples, a successful approach has been to use an iterative process, starting with a seeding set to which the whole dataset is aligned, a new reference is then extracted and used as a seed in the next iteration; a procedure repeated until convergence. To overcome the inter-individual variability in samples with \u0026ldquo;similar inter-individual organ structure but differences in cell type location and organ dimensions\u0026rdquo; a certain degree of supervision was added to the procedure; by manual annotation of landmarks etc. Semi-supervised processes are also predicted to be of great use in the construction of a human atlas. For highly non-stereotypical samples, there were no good strategies for template construction at the date of their writing, and they mention how this will have to be remedied by new methodological developments.\nInterestingly the authors highlight a fourth orthogonal, and fairly unconventional, approach that discards the use of a pre-defined coordinate system, landmarks and templates, in favor of learning it from the data itself. Multiple examples of methods for spatial reconstruction (see Seurat v3 and novoSpaRc) can be seen as testimonies to spatial information being contained within the expression profiles of cells. To me, this is very similar to the SLAM problem in robot localization, where one tries to create the map while also finding ones current position within it.\nThey also discuss how, once a CCF is established, samples of the same and different modalities may be mapped to it. If we are operating with data of the same modality, the process should be nothing but straightforward. We simply use the same strategy as when creating the CCF itself. For other cross-modality-mapping the case is slightly different, and integration will only be possible if the assumption that both modalities shares a latent representation (e.g., chromatin accessibility) holds.\nNow, one idea that the authors introduce, and which according to them is novel (I can find nothing that would contradict the claim) is that given all these challenges and how much influence a sample\u0026rsquo;s character has on the choice of method, is to dismiss the objective of creating a single CCF. Instead, multiple CCFs should be created in a hierarchical (by scale) fashion, which would allow both horizontal and vertical movements across the atlas, but where each CCF is adapted to best fit the data. This would represent a sort of \u0026ldquo;atlas of atlases\u0026rdquo; as they express it. To me, this makes perfect sense, although it would have been interesting to see some more discussion of how the vertical movements (between scales) were envisioned.\nTheir conclusions are brief, and so shall mine be. They make a strong argument for the value of CCFs, and why they deserve our attention. Inter-individual variability is the biggest hurdle to tackle, but there are strategies for it, though highly dependent on the data; this is not a \u0026ldquo;one-size fits all\u0026rdquo; type of problem. When the data is complex and we can\u0026rsquo;t establish natural anatomical templates or find good landmarks, perhaps it\u0026rsquo;s better to let the data dictate its own reference, using approaches inspired from methods of spatial reconstruction. All in all, CCFs brings structure to a very chaotic and seemingly unstructured space - and they are imperative to the process of creating a unified atlas of the human body.\n","date":1611064897,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611064897,"objectID":"38dba622c205a2a69ebb198c9cc0e503","permalink":"/post/2021-01-19-ccf-rev/","publishdate":"2021-01-19T15:01:37+01:00","relpermalink":"/post/2021-01-19-ccf-rev/","section":"post","summary":"For quite some while now, I\u0026rsquo;ve carried a slight sensation of guilt within me. Guilt for being so caught up in my own projects and activities that I\u0026rsquo;ve come to neglect one of the most essential duties as a researcher: to survey, read, and contemplate upon new (and old) material that have been published.","tags":["review","CCF","common coordinate framework"],"title":"Paper Review | Toward a Common Coordinate Framework for the Human Body","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;m thrilled to announce that a collaboration with my good friend and colleague, Franziska Hildebrandt, now has matured to the state of being ready for journal submission, which also coincides with a pre-print upload to bioRxiv.\nThe pre-print is titled \u0026ldquo;Spatial Transcriptomics to define transcriptional patterns of zonation and structural components in the liver\u0026rdquo;, and focus on the application of ST2K (similar to the old legacy ST1K and the newer Visium platform). It is the first time that ST (Spatial Transcriptomics) has been applied to the mouse liver, and aims to show how the technique enables the liver to be explored from new, different and illuminating perspective. A lot of standard analysis, such as clustering and identification of differential genes (DEGs) are presented in the paper and show affirmative results, attesting to the (experimental) method\u0026rsquo;s ability to delineate regions with various expression profiles (often overlapping well with morphological structures).\nMy contribution to this paper was nevertheless slightly different and consisted mainly of two parts:\n Construction of a framework to model feature signals as a function of the distance to a given object. Development of a classifier to predict vein type (a binary classification task) based on the expression profiles surrounding a certain genes.  Both concepts are excellent examples of analyses that are exclusive to spatial data, i.e., similar information cannot be obtained from single cell experiments; something I\u0026rsquo;m keen to highlight, as it is often the case that one fails to make proper use of the \u0026ldquo;spatial\u0026rdquo; part in spatial transcriptomics data, and rather treat it as single cell data, to then visualize the results spatially. Both implementations are very simple, but I have not seen similar analyses in ST or Visium data yet, of course that could be due to me not having surveyed the field thoroughly enough, but still. Allow me to elaborate some on these two concepts below.\nFeature by distance In the liver tissue one can broadly classify vein structures as either portal or central based on their position and connections. Previous studies have shown that these structures are associated with slightly different expression profiles, as different celltypes (periportal respective pericentral hepatocytes) are present in them.\nWhat we did in this study was to first \u0026ldquo;measure\u0026rdquo; the distance of each spot (spatial capture location) to the nearest vein (of respective type), and then for each gene create a set of distance-expression tuples that could be visualized in a scatter plot. While these values are discrete, once a curve smooting techniques is applied we can easily think of the resulting curve as a function that maps distance to gene expression. By doing so one could really see how the associated marker genes (to each vein) had a high expression near their respective vein, and how this decreased as distance increased (inverse relationship). You can see some examples of these trends in the plots below, top row displays portal vein marker genes, bottom central:\nClassification of vein types by expression Now, while pathologists and experts in the field usually are able to annotate each vein type by visual inspection (this is how we obtained the annotations used for the feature by distance plots), there are sometimes ambiguities and limited access to such individuals. However, the expression profiles of portal and central veins should be the similar for all veins of the same type, independent of whether identity is easy to deduce from morphology or not. Hence, by creating neighborhood expression profiles (NEPs) surrounding each vein, and training a classifier (actually logistic regression) using NEPs from veins with known identities; we were able to predict vein type of ambiguous veins. During cross-validation (leave one sample out) the model also performed very well with higher than 80% accuracy.\nTo read more about the study, check out the manuscript here, and the Python package we released (hepaquery) to perform similar analyses with any type of spatial transcriptomics data at the github page.\nI\u0026rsquo;m very happy to have worked with a nice team with complementary biology and computational skills, this really resulted in a neat manuscript; also a big kudos to the first author Franziska, who pushed very hard in the end to finalize this project.\n","date":1610449159,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610449159,"objectID":"fd7e03d9d0f710dfa8d11e7f9a57ef2d","permalink":"/post/2021-01-12-liver-biorxiv/","publishdate":"2021-01-12T11:59:19+01:00","relpermalink":"/post/2021-01-12-liver-biorxiv/","section":"post","summary":"I\u0026rsquo;m thrilled to announce that a collaboration with my good friend and colleague, Franziska Hildebrandt, now has matured to the state of being ready for journal submission, which also coincides with a pre-print upload to bioRxiv.","tags":[],"title":"Manuscript | Spatial Transcriptomics to define transcriptional patterns of zonation and structural components in the liver","type":"post"},{"authors":["Alma Anderson"],"categories":[],"content":"I\u0026rsquo;ve always found it appealing, or rather enticing, to study biological systems from a more formal and mathematical perspective; with the main objective to somehow be able to describe them with equations rather than words. Equations are exact, definite, universal, but foremost, they are predictive. The alluring property of mathematical models is that they do not only provide a description of our system, but they are also responsive, we may ask \u0026ldquo;if I replace parameter X with parameter Y, what will my system look like?\u0026quot;.\nOf course, our models - however beautiful or carefully constructed - are nothing but just that, models. And as the well known apohorism states, \u0026ldquo;all models are wrong\u0026rdquo;. While this statement is true, it is far from detrimental. Sometimes, being wrong but within a certain marginal of error to the truth is just about good enough. When driving, none of us knows the exact distance to the other cars, or how much a push on the gas will accelerate our car, still (most of the time) we manage to zig-zag through the densely packed lanes without crashing into each other - all from the simple incorrect model we constructed in our minds. So, yes - models are wrong, but they are most certainly useful and tremendously valuable in our quest to understand and analyze biological systems.\nIt was with this interest - and a requirement for PhD students at KTH to collect 60 ECTS credits - that I enrolled for a course in Applied Estimation. In short, the course focus on how to estimate states or parameters from noisy measurements such as empirical data. Albeit designed for people working with robotics, the concepts are indubitably applicable to biology - where noise is the rule rather than the exception.\nAs a part of the course, we had to devise our own project (where one estimation method was to used) and implement it in code. My choice fell upon a subject (and method) that I have little previous experience with: automated cell tracking in brightfield images, using GM-PHD (Gaussian Mixture Probability Hypothesis Density) filters. The theory is very interesting, and I was particularly intrigued by the concept of Random Finite Sets, here used to track multiple objects simultaneously. The method shares many similarities with the Kalman Filter, but rather than propagating a single state, the whole set is propagated in time. Easily explained, one might think of it as propagating a Gaussian Mixture in time, where each component represents a peak or cell. The original publication by Vo and Ma outlines the details of the GM-PHD filter better than I could ever attempt to, and I would refer anyone with an interest to have a close look at it.\nAs for the final product, I implemented the algorithm proposed by Vo and Ma (Table I-III) in python, with an easy to use CLI; allowing anyone who might want to try it out to do so fairly (I hope) seamlessly. Instructions and examples of usage can be found at the github page. The animated image above is the result of applying this implementation to a set of \u0026ldquo;HeLa cells stably expressing H2b-GFP\u0026rdquo;, downloaded from celltrackingchallenge.net. The implementation is far from perfect, and much work remains to even have a chance of competing with the more sophisticated alternatives. Still, for a couple of days of work, the performance is not too bad - and it was a fun experience implementing it.\nThe course has prompted a lot of new ideas and really allowed me to see some old problems from a completely different lens. For example, I believe a lot of the ideas found in SLAM (Simultaneous Localization and Mapping) could be used when working with questions related to trajectory inferenc, where we partly want to assess the developmental landscape but also map a path through it. It\u0026rsquo;s always a very rewarding experience to enter a completely new field, where people come from a very different background - in my opinion this intentional \u0026ldquo;push\u0026rdquo; out of the comfort zone really has a tendency to spark great exciting ideas, however uncofortable it feels at first.\n","date":1610097951,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610097951,"objectID":"bd2c4610723e1a9d61a3ef3498fcc267","permalink":"/post/2021-01-08-celltracker/","publishdate":"2021-01-08T10:25:51+01:00","relpermalink":"/post/2021-01-08-celltracker/","section":"post","summary":"I\u0026rsquo;ve always found it appealing, or rather enticing, to study biological systems from a more formal and mathematical perspective; with the main objective to somehow be able to describe them with equations rather than words.","tags":[],"title":"Models and automated cell tracking","type":"post"},{"authors":[],"categories":null,"content":"","date":1605884100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605884100,"objectID":"5ccf3a9fac565207aef25af7914f7876","permalink":"/talk/2020-11-20-ugm/","publishdate":"2020-11-20T16:25:00+01:00","relpermalink":"/talk/2020-11-20-ugm/","section":"talk","summary":"","tags":[],"title":"Exploring the cell type topography by integration of spatial and single cell data","type":"talk"},{"authors":[],"categories":null,"content":"","date":1603359000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603359000,"objectID":"ff44752e2eefe605bb3d2e05721fef75","permalink":"/talk/2020-10-22-10xrs/","publishdate":"2020-10-22T11:00:00+01:00","relpermalink":"/talk/2020-10-22-10xrs/","section":"talk","summary":"","tags":[],"title":"Exploring the cell type topography by integration of spatial and single cell data","type":"talk"},{"authors":["Alma Anderson"],"categories":[],"content":"Delighted to say that our manuscript \u0026ldquo;Single-cell and spatial transcriptomics enables probabilistic inference of cell type topography\u0026rdquo; is now published in Communications Biology and thus out there for everyone to read in its final form. This is the paper that describes the theoretical underpinnings for stereoscope, which allows you to spatial map cell types found in single cell data onto spatial transcriptomics data. There is also a nice bit of discussion as to whether spatial transcriptomics data (ST/Visium) can be properly modeled using a negative binomial distribution (which we use) found in the supplementary (spoiler, it seems like it). Have a look at it if you are interested!\n manuscript link : HERE stereoscope link : HERE  P.S: I\u0026rsquo;m also planning to update the code base in the future, providing an API for scanpy, but also add some features like subsampling and gene selection to the standard modules; but that is for later.\n","date":1602237904,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602237904,"objectID":"94664ec0002f5a54210a1bf6745bbbe7","permalink":"/post/stereoscope-001/initial-post/","publishdate":"2020-10-09T11:05:04+01:00","relpermalink":"/post/stereoscope-001/initial-post/","section":"post","summary":"Delighted to say that our manuscript \u0026ldquo;Single-cell and spatial transcriptomics enables probabilistic inference of cell type topography\u0026rdquo; is now published in Communications Biology and thus out there for everyone to read in its final form.","tags":["publications","single cell mapping","stereoscope"],"title":"Publication | Spatial Single Cell Mapping","type":"post"},{"authors":["Alma Anderson"],"categories":[],"content":"We\u0026rsquo;ve put out a new manuscript on bioRxiv, where the expression landscape of HER2-positive breast cancer tumors is studied from a spatial perspective. This study contains several examples of how spatial data may be analyzed and what information beyond the obvious - like the spatial distribution of a certain gene - that can be mined from it. My hope is that anyone looking for ideas regarding how to analyze their own spatial data find this useful and relevant, irregardles of their interest in breast cancer. In the study we cluster spots by gene expression, but also take this one step further, using the clusters to find core signatures of immune and tumor cell populations within our samples. We also use stereoscope to map single cell data down onto our tissue sections, information which then is used to indentify potential TLS-sites as well as to study patterns of cell type co-localization. All the scripts and a neat app for visualization of the results can be found here.\n","date":1594807504,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594821904,"objectID":"2201b458b56f3a4235bf472b7c194dfd","permalink":"/post/her2-001/initial-post/","publishdate":"2020-07-15T11:05:04+01:00","relpermalink":"/post/her2-001/initial-post/","section":"post","summary":"We\u0026rsquo;ve put out a new manuscript on bioRxiv, where the expression landscape of HER2-positive breast cancer tumors is studied from a spatial perspective. This study contains several examples of how spatial data may be analyzed and what information beyond the obvious - like the spatial distribution of a certain gene - that can be mined from it.","tags":[],"title":"Manuscript | Spatial Exploration of Her2 Breast Cancer","type":"post"},{"authors":["Alma Anderson"],"categories":[],"content":"Browsing through my daily feed, I stumbled upon this neat resource which lists (and links to) fundamental ideas that have shaped the field of systems biology. To me, many of these publications represent some of the earliest attempts to reduce complex biological systems and processes into something simpler, more comprehensible, by viewing them through the lense of mathematics; truly groundbreaking efforts that paved the way for the current trends of ML/AI in biology. I\u0026rsquo;d strongly recommend anyone with an interest in the field to have a look at it; something that contains work from both Alan Turing and John von Neumann can\u0026rsquo;t be anything but good!\n","date":1593594304,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593594304,"objectID":"3ecfac006474e62c5ea80b86b638f81e","permalink":"/post/synt-bio-001/initial-post/","publishdate":"2020-07-01T10:05:04+01:00","relpermalink":"/post/synt-bio-001/initial-post/","section":"post","summary":"Browsing through my daily feed, I stumbled upon this neat resource which lists (and links to) fundamental ideas that have shaped the field of systems biology. To me, many of these publications represent some of the earliest attempts to reduce complex biological systems and processes into something simpler, more comprehensible, by viewing them through the lense of mathematics; truly groundbreaking efforts that paved the way for the current trends of ML/AI in biology.","tags":[],"title":"Resource | Formal systems in Biology","type":"post"},{"authors":[],"categories":null,"content":"","date":1591189200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591189200,"objectID":"8e68e4ef91d573ebdff6a6a5aba7e08e","permalink":"/talk/2020-06-03-symp/","publishdate":"2020-06-03T16:00:00+01:00","relpermalink":"/talk/2020-06-03-symp/","section":"talk","summary":"","tags":[],"title":"Exploring the cell type topography by integration of spatial and single cell data","type":"talk"},{"authors":[],"categories":null,"content":"","date":1590768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590768000,"objectID":"330e1e85b390d61cfdda5cf579d9f4e9","permalink":"/talk/2020-05-29-scadv/","publishdate":"2020-05-29T17:00:00+01:00","relpermalink":"/talk/2020-05-29-scadv/","section":"talk","summary":"","tags":[],"title":"Computational Analysis of Spatial Transcriptomics Data","type":"talk"},{"authors":[],"categories":[],"content":"I recently compiled some exercises focusing on analysis of spatial data for a course (hosted by SIB). The material spans from very basic analysis (visualization and clustering) to inference of spatial cell type distributions by integration of single cell and spatial data. If you are excited about spatial transcriptomics and what type of analysis you can apply to this type of data, check out the material here . It\u0026rsquo;s all written using standard python libraries for maximal transparency, allowing you to follow each and every step.\n","date":1590761104,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590761104,"objectID":"81bf1cb397776fe1e1fb0378851b2df6","permalink":"/post/init/initial-post/","publishdate":"2020-05-29T15:05:04+01:00","relpermalink":"/post/init/initial-post/","section":"post","summary":"I recently compiled some exercises focusing on analysis of spatial data for a course (hosted by SIB). The material spans from very basic analysis (visualization and clustering) to inference of spatial cell type distributions by integration of single cell and spatial data.","tags":[],"title":"Exercises | Spatial Transcriptomics Data Analysis","type":"post"},{"authors":[],"categories":null,"content":"","date":1587553783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587553783,"objectID":"61f427c79d2e31f65b5acc1f1fe71e3d","permalink":"/talk/2020-04-22-emea/","publishdate":"2020-04-22T12:09:43+01:00","relpermalink":"/talk/2020-04-22-emea/","section":"talk","summary":"","tags":[],"title":"Exploring the cell type topography by integration of single cell and spatial transcriptomics data","type":"talk"}]